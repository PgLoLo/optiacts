{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47131464-3e79-47dc-95fc-293fb5c88a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import optiacts\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training  # , PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96be2ec-5557-4ced-8605-eaac31c71256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats():\n",
    "    stats = torch.cuda.memory_stats()\n",
    "    print(f'allocated: {stats[\"active_bytes.all.current\"] / 2**30:.3}Gb, peak: {stats[\"active_bytes.all.peak\"] / 2**30:.3}Gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226eaa1d-743c-4a2e-9ba2-5b8c3225be21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f266b974c2426ca701dc97f9c66f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "\n",
    "\n",
    "base_model = \"mistralai/Mistral-7B-v0.1\"\n",
    "new_model = \"test-mistral-7B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "def run():\n",
    "    out = model(torch.randint(0, 10000, [1, 2**12], device='cuda'))\n",
    "    print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12a7b0d7-8342-4418-a4cd-2f092b6e3104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size:\n",
      "allocated: 4.83Gb, peak: 5.08Gb\n"
     ]
    }
   ],
   "source": [
    "print('Model size:')\n",
    "print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ac5a11-30a5-4e4c-91fa-f8983797180b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard activations:\n",
      "allocated: 47.5Gb, peak: 47.6Gb\n"
     ]
    }
   ],
   "source": [
    "print('Standard activations:')\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d61ae2-dfab-43de-9aa9-c9366919127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory with optiacts:\n",
      "allocated: 40.8Gb, peak: 40.8Gb\n"
     ]
    }
   ],
   "source": [
    "for layer in model.model.layers:\n",
    "    layer.mlp.act_fn = optiacts.GELU()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "print('Memory with optiacts:')\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "063d76b92d9747c8ab918d44e82c3bf5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "15b86eacf3c34d959b23d93c181d4437": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "36f266b974c2426ca701dc97f9c66f90": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_acdbd1ee49144fd0a19dd6d76c83b90f",
        "IPY_MODEL_eca072e0ff5e456db6afa30b6fdb44c7",
        "IPY_MODEL_a2a62f5c5b664de6ac30afa2e3f57913"
       ],
       "layout": "IPY_MODEL_063d76b92d9747c8ab918d44e82c3bf5"
      }
     },
     "700a4424e48546cdad1267cb20d22e85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a2a62f5c5b664de6ac30afa2e3f57913": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_bd9c8ba193774359a63eeae7774b0730",
       "style": "IPY_MODEL_f2f26d963fbc41f68152c77e2c0138dd",
       "value": " 2/2 [00:03&lt;00:00,  1.56s/it]"
      }
     },
     "acdbd1ee49144fd0a19dd6d76c83b90f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_700a4424e48546cdad1267cb20d22e85",
       "style": "IPY_MODEL_b145edf52eb1419caef129f1bd8e44a2",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "b145edf52eb1419caef129f1bd8e44a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bd9c8ba193774359a63eeae7774b0730": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "db4def5b0bd3436fa8853c60daea05aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "eca072e0ff5e456db6afa30b6fdb44c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_15b86eacf3c34d959b23d93c181d4437",
       "max": 2,
       "style": "IPY_MODEL_db4def5b0bd3436fa8853c60daea05aa",
       "value": 2
      }
     },
     "f2f26d963fbc41f68152c77e2c0138dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
